---
title: "Environmental Data Bias: Case Studies and Pathways to Justice"
author: Joshua Ferrer-Lozano
date: 12/9/2025
format: html
editor: visual
---

# Introduction

Environmental data are often treated as neutral, objective, and “raw.” Yet as scholars and professionals across environmental justice, data studies, and Indigenous methodologies have shown, data are always shaped by the contexts in which they are collected, reported, and interpreted. Biases-whether through suppression, measurement choices, deficit framing, sampling gaps, or confounding variables-can influence how environmental problems are understood and addressed. Recognizing these biases is essential for building more equitable, transparent, and participatory data practices. This project examines five case studies of environmental data bias, each illustrating a distinct type of bias and its consequences for science, policy, and justice. Together, they demonstrate that environmental data justice requires not only technical expertise but also attention to social, cultural, and political contexts.

# Case Descriptions

## Archiving Federal Environmental Data (Nost et al., 2024)

The Environmental Data & Governance Initiative (EDGI) documented how the Trump administration deleted Clean Power Plan webpages, calculators, and fact sheets months before formally proposing repeal. This is a clear example of reporting bias and suppression, where inconvenient data were removed from public view. The impact was significant where communities and policymakers lost access to tools that quantified emissions and incentives, undermining transparency and accountability. Archiving efforts like EDGI’s highlight the importance of preserving not only data sets but also their metadata and narratives.

## Louisiana Bucket Brigade (Walker et al., 2018; Vera et al., 2019)

In Louisiana, state air monitors averaged toxic emissions over 24‑hour periods, masking short‑term spikes in pollution near chemical facilities. Community members responded by developing community “air buckets” to capture peak exposures. This case illustrates measurement bias, where methodological choices obscured hazards. Official data understated health risks, while community‑generated data revealed the lived realities of fence‑line communities-communities that are immediately adjacent to highly polluting facilities. The Bucket Brigade demonstrates how participatory monitoring can counteract these biases and disseminate environmental knowledge.

## Indigenous Health Statistics (Walter & Suina, 2019)

Indigenous statistics in settler nation states often emphasize difference, disparity, disadvantage, dysfunction, and deprivation—the so‑called “5D data.” This framing positions Indigenous peoples primarily through deficit narratives, a form of outcome reporting bias and deficit framing bias. The implication is that this data reinforces colonial logics/frameworks and obscure Indigenous lifeworlds of wellness, which include spirituality, language, and community well‑being. Walter and Suina argue for specifically Indigenous quantitative methodologies and data sovereignty to disrupt these deficit narratives and ensure the data reflect Indigenous priorities and values.

## Biodiversity Monitoring (Young et al., 2014)

The global biodiversity data sets disproportionately represent birds and mammals, while insects, fungi, and plants are under counted. This is a classic case of sampling bias, where certain species are "privileged" or prioritized in monitoring efforts. The impact is unbalanced conservation priorities that neglect less visible but ecologically critical organisms. Young and colleagues emphasize the need for interdisciplinary dialogue and more inclusive monitoring strategies that reflect the complexity of the ecosystems and the relevance of policy .

## Biases in Environmental Causal Inference (Konno et al., 2024)

This journal identified 121 types of bias relevant to environmental research, including confounding and selection bias. For example, in agricultural studies, management practices may confound relationships between soil moisture, sunlight, and crop yields. Even well‑designed studies can mislead if variables are not accounted for. Konno and colleagues highlight the importance of utilizing critical appraisal tools to identify and mitigate biases in causal inference, ensuring that environmental interventions are based on valid evidence.

# Reflections

These five case studies illustrate that environmental data bias is not simply a technical flaw but a reflection of deeper social, political, and cultural contexts. Each case demonstrates how decisions about what data to collect, how to measure it, and how to report it are shaped by power relations and institutional assumptions/priorities.

From Nost et al. (2024), we see how political suppression of data undermines transparency and accountability. This resonates with my own experience in Georgia, where language bias limited access to environmental reports. Even when data were technically available, the choice of language shaped who could meaningfully engage with it and the cultural context that may be focused. Both examples highlight that accessibility is not just about open data sets but about ensuring that communities can interpret and use information.

The Louisiana Bucket Brigade shows how measurement bias can obscure lived realities. Official monitors averaged emissions over long periods, masking acute exposures. In Hawaii, I learned through the practice of “talk story” that technical jargon without relational communication can alienate communities and create distrust between institution/government and its constituents. Just as the Bucket Brigade’s air buckets made data relatable and actionable, “talk story” emphasizes that data must be shared in ways that build trust and resonate with local values.

Walter & Suina (2019) remind us that deficit framing bias can reproduce colonial narratives. Indigenous health statistics often emphasize disparities without reflecting Indigenous lifeworlds of wellness. This connects to my vineyard LiDAR project, where focusing only on quantitative canopy metrics risks ignoring cultural practices of viticulture. Both cases show that data must be situated in broader lifeworlds to avoid reinforcing narrow or harmful narratives.

Young et al. (2014) demonstrate sampling bias in biodiversity monitoring, where charismatic species are over represented. This reflects a broader tendency to privilege what is visible or valued by dominant institutions. It raises the question: whose priorities shape data collection? Similarly, in my own work, privileging certain variables (sunlight, soil moisture) without considering management practices risks reproducing sampling bias in agricultural contexts.

Finally, Konno et al. (2024) highlight confounding and selection biases in causal inference. Their systematic review shows that even well‑designed studies can mislead if confounding variables are not accounted for. This reinforces the importance of reproducibility and transparency in my own regression analyses, where vineyard management practices may confound relationships between environmental variables and grape sugar content.

Together, these reflections underscore the importance of frameworks like the CARE Principles and Environmental Data Justice (EDJ). CARE emphasizes collective benefit, authority to control, responsibility, and ethics, ensuring that data practices respect Indigenous sovereignty and community values. EDJ situates data within social and political contexts, challenging extractive logics and deficit narratives. Both frameworks remind us that environmental data are never raw—they are always relational, situated, and shaped by power.

# Solutions

The five case studies demonstrate that environmental data bias arises from multiple sources—political suppression, measurement choices, deficit framing, sampling gaps, and confounding variables. Addressing these biases requires a combination of technical, social, and ethical strategies. Revisiting the mentioned case-studies, solutions that may address each unique bias include:

Countering Suppression Bias (Nost et al., 2024): Establishing independent archiving initiatives that preserve data sets, metadata, and contextual narratives would be key. Then model cards and datasheets that provide transparency about the origins, limitations, and intended uses.

Addressing Measurement Bias (Louisiana Bucket Brigade): Ensure we incorporate participatory monitoring methods that allow communities to collect data reflecting their lived experiences. Grassroots projects and low‑cost sensors can complement official monitoring.

Disrupting Deficit Framing (Walter & Suina, 2019): We can adopt Indigenous quantitative methodologies and apply the CARE Principles to ensure sovereignty and empowerment.

Reducing Sampling Bias (Young et al., 2014): Start by expanding biodiversity monitoring to include underrepresented species and ecosystems. Moreover, include interdisciplinary collaboration to frame research questions.

Mitigating Confounding and Selection Bias (Konno et al., 2024): Include the use of critical appraisal tools to systematically identify and address biases in causal inference. Encourage and administer transparency in methods, reproducibility of analyses, and reporting of any confounding variables.

Integrative strategies include combining FAIR and CARE Principles to ensure data are both technically objective and ethically governed, applying Environmental Data Justice to place data within social, political contexts, and communicating data through culturally appropriate practices such as “talk story” or multilingual reporting. These approaches ensure that environmental data are not only accurate but also meaningful, accessible, and just.

# Conclusion

Environmental data bias will continue to be present in human designed methodologies, data and political/cultural environments. By recognizing the forms of bias like suppression, measurement, deficit framing, sampling, and confounding, we can design strategies that make data more transparent, participatory, and equitable. The five case studies presented here demonstrate that environmental data justice requires both technical expertise and social responsibility. Ultimately, data is never raw and that they are always cooked with care. The challenge is to ensure that they are cooked in ways that serve communities, respect sovereignty, and advance environmental justice.

# References

Konno, K., Gibbons, J., Lewis, R., & Pullin, A. S. (2024). Potential types of bias when estimating causal effects in environmental research and how to interpret them. Environmental Evidence, 13(1), 1–31. https://doi.org/10.1186/s13750-024-00324-7

Nost, E., Gehrke, G., Vera, L., & Hansen, S. (2024). Why the Environmental Data & Governance Initiative is archiving public environmental data. Patterns, 6(1), 101151. https://doi.org/10.1016/j.patter.2024.101151

Walker, D., Nost, E., Lemelin, A., Lave, R., & Dillon, L. (2018). Practicing environmental data justice: From DataRescue to Data Together. Geo: Geography and Environment, 5(2), e00061. https://doi.org/10.1002/geo2.61

Walter, M., & Suina, M. (2019). Indigenous data, indigenous methodologies and indigenous data sovereignty. International Journal of Social Research Methodology, 22(3), 233–243. https://doi.org/10.1080/13645579.2018.1531228

Vera, L. A., Walker, D., Murphy, M., Mansfield, B., Mohamed Siad, L., Ogden, J., & Environmental Data & Governance Initiative (EDGI). (2019). When data justice and environmental justice meet: Formulating a response to extractive logic through environmental data justice. Information, Communication & Society, 22(7), 1012–1028. https://doi.org/10.1080/1369118X.2019.1596293